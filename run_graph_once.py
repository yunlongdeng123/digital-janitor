#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Digital Janitor - LangGraph å·¥ä½œæµç‰ˆ

æ ¸å¿ƒåŠŸèƒ½ï¼š
- ğŸ”„ LangGraph çŠ¶æ€å›¾å·¥ä½œæµ
- ğŸ¤ HITL (Human-in-the-Loop) äººå·¥ç¡®è®¤
- ğŸ“¦ çœŸå®æ–‡ä»¶ç§»åŠ¨ + å†²çªå¤„ç†
- ğŸ“ å®Œæ•´æ—¥å¿—è¿½è¸ª

Step 6 é‡æ„ï¼š
- å°è£… JanitorWorkflow ç±»ï¼Œä½¿æ ¸å¿ƒé€»è¾‘å¯è¢«å¤–éƒ¨è°ƒç”¨
- æ—¢å¯ä½œä¸ºå‘½ä»¤è¡Œå·¥å…·è¿è¡Œï¼Œä¹Ÿå¯ä½œä¸ºåº“è¢«å¯¼å…¥

ä½¿ç”¨æ–¹å¼ï¼š
    1. å‘½ä»¤è¡Œæ¨¡å¼ï¼š
       python run_graph_once.py --limit 5 --execute
    
    2. ä½œä¸ºåº“å¯¼å…¥ï¼š
       from run_graph_once import JanitorWorkflow
       workflow = JanitorWorkflow()
       result = workflow.process_file(Path("inbox/doc.pdf"))
"""

from __future__ import annotations

import sys
import json
import argparse
import re
import os
from pathlib import Path
from datetime import datetime
from typing import TypedDict, Optional, Dict, Any

# è®¾ç½® UTF-8 ç¼–ç æ”¯æŒ (Windows æ§åˆ¶å°)
if sys.platform == "win32":
    os.environ["PYTHONIOENCODING"] = "utf-8"
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8')
    if hasattr(sys.stderr, 'reconfigure'):
        sys.stderr.reconfigure(encoding='utf-8')

import yaml
from dotenv import load_dotenv

# LangGraph æ ¸å¿ƒç»„ä»¶
from langgraph.graph import StateGraph, END

# é¡¹ç›®å†…éƒ¨æ¨¡å—
from core.schemas import RenamePlan
from core.validator import validate_plan
from utils.file_ops import discover_files, extract_text_preview_enhanced, get_file_size_mb, safe_move_file
from core.llm_processor import analyze_file

# Memory ç³»ç»Ÿ
from core.memory import MemoryDatabase, ApprovalRepository, PreferenceRepository
import hashlib

# --- è¾…åŠ©å‡½æ•°ï¼šPR#2 ä¸‰çº§è·¯ç”±ç­–ç•¥ ---

# ç±»åˆ«ä¸­æ–‡æ˜ å°„è¡¨
CAT_CN_MAP = {
    "invoice": "å‘ç¥¨",
    "receipt": "ç¥¨æ®",
    "bank_statement": "é“¶è¡Œè´¦å•",
    "contract": "åˆåŒ",
    "paper": "è®ºæ–‡",
    "image": "å›¾ç‰‡",
    "presentation": "æ¼”ç¤ºæ–‡ç¨¿",
    "default": "å…¶ä»–",
}

# æ— æ•ˆ Vendor å€¼åˆ—è¡¨ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
INVALID_VENDOR_VALUES = {"", "unknown", "n/a", "none", "æ— ", "æœªçŸ¥", "null"}


def sanitize_path_component(value: str) -> str:
    """
    æ¸…æ´—è·¯å¾„ç»„ä»¶ï¼Œç§»é™¤éæ³•å­—ç¬¦
    
    Args:
        value: åŸå§‹å€¼ï¼ˆå¦‚ vendor åç§°ï¼‰
    
    Returns:
        å®‰å…¨çš„è·¯å¾„ç»„ä»¶å­—ç¬¦ä¸²
    """
    if not value:
        return ""
    # æ›¿æ¢ Windows/Unix è·¯å¾„éæ³•å­—ç¬¦
    illegal_chars = r'[/\\:*?"<>|]'
    sanitized = re.sub(illegal_chars, '_', value)
    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹ï¼ˆWindows ä¸å…è®¸æ–‡ä»¶å¤¹åä»¥ç‚¹ç»“å°¾ï¼‰
    sanitized = sanitized.strip().rstrip('.')
    return sanitized


def is_valid_vendor(vendor: Optional[str]) -> bool:
    """
    æ£€æŸ¥ vendor æ˜¯å¦æœ‰æ•ˆ
    
    Args:
        vendor: ä¾›åº”å•†/å¯¹æ–¹åç§°
    
    Returns:
        True å¦‚æœ vendor æœ‰æ•ˆï¼ŒFalse å¦åˆ™
    """
    if vendor is None:
        return False
    vendor_lower = vendor.strip().lower()
    return vendor_lower not in INVALID_VENDOR_VALUES


def build_date_partition_path(category: str, date_str: Optional[str]) -> str:
    """
    æ„å»ºæ—¥æœŸåˆ†å±‚è·¯å¾„ (Category/YYYY/MM)
    
    ç”¨äºè´¢åŠ¡ç±»æ–‡æ¡£ï¼ˆå‘ç¥¨ã€ç¥¨æ®ã€é“¶è¡Œè´¦å•ï¼‰çš„å½’æ¡£ã€‚
    
    Args:
        category: æ–‡æ¡£ç±»åˆ«
        date_str: æå–çš„æ—¥æœŸå­—ç¬¦ä¸² (æ ¼å¼: YYYY-MM æˆ– YYYY-MM-DD)
    
    Returns:
        ç›®æ ‡ç›®å½•è·¯å¾„ï¼Œå¦‚ "å‘ç¥¨/2024/12"
    """
    # è·å–ä¸­æ–‡ç±»åˆ«å
    cat_cn = CAT_CN_MAP.get(category, CAT_CN_MAP["default"])
    
    # è§£ææ—¥æœŸ
    year, month = "æœªçŸ¥å¹´ä»½", "æœªçŸ¥æœˆä»½"
    if date_str:
        m = re.match(r"(?P<y>20\d{2})[-./]?(?P<m>\d{2})?", date_str)
        if m:
            year = m.group("y")
            month = m.group("m") if m.group("m") else "01"
    
    return f"{cat_cn}/{year}/{month}"


def build_semantic_path(
    category: str, 
    vendor: Optional[str],
    default_tpl: str = "{category}/{vendor}",
    fallback_tpl: str = "{category}/General"
) -> str:
    """
    æ„å»ºè¯­ä¹‰åŒ–è·¯å¾„ (Category/Vendor)
    
    ç”¨äºéè´¢åŠ¡ç±»æ–‡æ¡£ï¼ˆåˆåŒã€è®ºæ–‡ç­‰ï¼‰çš„å½’æ¡£ã€‚
    
    Args:
        category: æ–‡æ¡£ç±»åˆ«
        vendor: ä¾›åº”å•†/å¯¹æ–¹/ä½œè€…åç§°
        default_tpl: é»˜è®¤æ¨¡æ¿ (å½“ vendor æœ‰æ•ˆæ—¶ä½¿ç”¨)
        fallback_tpl: å…œåº•æ¨¡æ¿ (å½“ vendor æ— æ•ˆæ—¶ä½¿ç”¨)
    
    Returns:
        ç›®æ ‡ç›®å½•è·¯å¾„ï¼Œå¦‚ "åˆåŒ/è…¾è®¯ç§‘æŠ€" æˆ– "åˆåŒ/General"
    """
    # è·å–ä¸­æ–‡ç±»åˆ«å
    cat_cn = CAT_CN_MAP.get(category, CAT_CN_MAP["default"])
    
    # æ£€æŸ¥ vendor æœ‰æ•ˆæ€§
    if is_valid_vendor(vendor):
        # ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        safe_vendor = sanitize_path_component(vendor)
        return default_tpl.format(category=cat_cn, vendor=safe_vendor)
    else:
        # ä½¿ç”¨å…œåº•æ¨¡æ¿
        return fallback_tpl.format(category=cat_cn)


# --- 1. å®šä¹‰çŠ¶æ€ (State) ---
class JanitorState(TypedDict, total=False):
    """å®šä¹‰å·¥ä½œæµä¸­æµè½¬çš„æ•°æ®ç»“æ„"""
    # è¾“å…¥
    file_path: Path           # æ–‡ä»¶è·¯å¾„
    cfg: dict                 # é…ç½®å­—å…¸
    archive_root: Path        # å½’æ¡£æ ¹ç›®å½•
    dry_run: bool             # æ˜¯å¦æ˜¯Dry-Run
    max_preview: int          # æœ€å¤§é¢„è§ˆå­—ç¬¦æ•°
    auto_approve: bool        # è‡ªåŠ¨æ‰¹å‡†æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•/æ¼”ç¤ºï¼‰
    preference_repo: Any      # ğŸ†• åå¥½ä»“åº“

    # ä¸­é—´äº§ç‰©
    preview: str              # é¢„è§ˆæ–‡æœ¬
    extraction_metadata: Dict[str, Any]  # ğŸ†• OCR V2: æ–‡æœ¬æå–å…ƒæ•°æ® (method, confidence, quality_score, needs_review, processing_time_ms)
    analysis: Dict[str, Any]  # å­˜å‚¨ LLM åˆ†æçš„åŸå§‹ç»“æœ
    plan: RenamePlan          # æ ¸å¿ƒå¯¹è±¡ï¼šé‡å‘½åè®¡åˆ’

    # ğŸ†• HITL å†³ç­–ç›¸å…³
    approved: bool            # æ˜¯å¦æ‰¹å‡†æ‰§è¡Œ
    decision: str             # å†³ç­–ç±»å‹ï¼šapproved / rejected / auto_reject_invalid / skipped / pending

    # ğŸ†• Step 5: æ–‡ä»¶ç§»åŠ¨ç›¸å…³
    move_result: Dict[str, Any]  # æ–‡ä»¶ç§»åŠ¨ç»“æœ

    # ğŸ†• Step 7: å¾…å®¡æ‰¹ç›¸å…³
    pending_file: str         # å¾…å®¡æ‰¹ JSON æ–‡ä»¶è·¯å¾„

    # ğŸ†• PR#2: è·¯ç”±ç­–ç•¥ç›¸å…³
    routing_source: str       # è·¯ç”±æ¥æº: memory / date_partition / semantic
    used_learned_preference: bool  # æ˜¯å¦ä½¿ç”¨äº†å­¦ä¹ åˆ°çš„åå¥½

    # è¾“å‡º/æ—¥å¿—
    record: Dict[str, Any]    # è®°å½•
    error: str                # é”™è¯¯ä¿¡æ¯


# --- 2. å®šä¹‰èŠ‚ç‚¹ (Nodes) ---

def node_extract_preview(state: JanitorState) -> JanitorState:
    """èŠ‚ç‚¹1: æå–æ–‡æœ¬é¢„è§ˆï¼ˆOCR V2 å¢å¼ºç‰ˆï¼‰"""
    fp = state["file_path"]  # è·å–æ–‡ä»¶è·¯å¾„
    max_preview = state.get("max_preview", 1000)
    
    # ğŸ†• OCR V2: ä½¿ç”¨å¢å¼ºç‰ˆæ–‡æœ¬æå–
    result = extract_text_preview_enhanced(fp, limit=max_preview)
    
    # æå–æ–‡æœ¬å†…å®¹
    state["preview"] = result.get("text", "")
    
    # ğŸ†• OCR V2: ä¿å­˜å…ƒæ•°æ®ï¼ˆæ’é™¤ text å­—æ®µï¼‰
    state["extraction_metadata"] = {
        "method": result.get("method", "unknown"),
        "confidence": result.get("confidence", 0.0),
        "quality_score": result.get("quality_score", 0),
        "needs_review": result.get("needs_review", False),
        "processing_time_ms": result.get("processing_time_ms", 0),
        "page_count": result.get("page_count", 0),
        "char_count": result.get("char_count", 0),
        "error": result.get("error"),
    }
    
    # ğŸ†• æ‰“å°ç®€çŸ­æ—¥å¿—
    method = state["extraction_metadata"]["method"]
    quality = state["extraction_metadata"]["quality_score"]
    time_ms = state["extraction_metadata"]["processing_time_ms"]
    cached = "_cached" in method
    
    print(f"   ğŸ“„ æ–‡æœ¬æå–: {method} | è´¨é‡={quality} | è€—æ—¶={time_ms}ms" + (" ğŸ’¾" if cached else ""))
    
    # å¦‚æœè´¨é‡è¾ƒä½ï¼Œæ‰“å°è­¦å‘Š
    if state["extraction_metadata"]["needs_review"]:
        print(f"   âš ï¸  OCR è´¨é‡è¾ƒä½ ({quality}åˆ†)ï¼Œå¯èƒ½éœ€è¦äººå·¥å®¡æŸ¥")
    
    return state


def node_llm_analyze(state: JanitorState) -> JanitorState:
    """èŠ‚ç‚¹2: è°ƒç”¨ LLM åˆ†æ"""
    fp = state["file_path"]
    try:
        # è°ƒç”¨æ ¸å¿ƒæ¨¡å—
        a = analyze_file(state.get("preview", ""), fp.name, max_preview=state.get("max_preview", 1000))
        
        # å°† Pydantic å¯¹è±¡è½¬ä¸º Dict å­˜å…¥ State (æ–¹ä¾¿åºåˆ—åŒ–)
        state["analysis"] = {
            "category": a.category,
            "confidence": a.confidence,
            "suggested_filename": a.suggested_filename,
            "extracted_date": a.extracted_date,
            "extracted_amount": a.extracted_amount,
            "vendor_or_party": a.vendor_or_party,
            "title": a.title,
            "rationale": a.rationale,
        }
    except Exception as e:
        state["error"] = f"LLM åˆ†æå¤±è´¥: {e}"
    return state


def node_build_plan(state: JanitorState) -> JanitorState:
    """
    èŠ‚ç‚¹3: æ„å»ºé‡å‘½åè®¡åˆ’ (RenamePlan)
    
    PR#2: å®ç°ä¸‰çº§è·¯ç”±ä¼˜å…ˆçº§
      1. Memory åå¥½ (æœ€é«˜) - ä»ç”¨æˆ·å†å²æ“ä½œä¸­å­¦ä¹ 
      2. æ—¥æœŸåˆ†å±‚è§„åˆ™ - è´¢åŠ¡ç±»æ–‡æ¡£æŒ‰ Category/YYYY/MM
      3. è¯­ä¹‰åŒ–å½’æ¡£ - å…¶ä»–æ–‡æ¡£æŒ‰ Category/Vendor
    """
    fp = state["file_path"]
    cfg = state["cfg"]

    # å¦‚æœä¹‹å‰çš„æ­¥éª¤æŠ¥é”™ï¼Œç”Ÿæˆä¸€ä¸ªå¤±è´¥çš„è®¡åˆ’
    if state.get("error"):
        plan = RenamePlan(
            category="error",
            new_name=fp.name,
            dest_dir="quarantine/failed",
            confidence=0.0,
            extracted={},
            rationale=state["error"],
            is_valid=False,
            validation_msg=state["error"],
        )
        state["plan"] = plan
        state["routing_source"] = "error"
        return state

    # æ­£å¸¸æ„å»ºé€»è¾‘
    a = state["analysis"]
    category = a["category"]
    vendor = a.get("vendor_or_party")
    date_str = a.get("extracted_date")
    
    # è¯»å–è·¯ç”±é…ç½®
    routing_cfg = cfg.get("routing", {})
    date_partition_types = routing_cfg.get("date_partition_types", ["invoice", "receipt", "bank_statement"])
    default_structure = routing_cfg.get("default_structure", "{category}/{vendor}")
    fallback_structure = routing_cfg.get("fallback_structure", "{category}/General")
    
    # === ä¸‰çº§è·¯ç”±ä¼˜å…ˆçº§ ===
    
    # 1ï¸âƒ£ ä¼˜å…ˆçº§ 1: Memory åå¥½ (æœ€é«˜)
    learned_folder = None
    if "preference_repo" in state and state["preference_repo"]:
        try:
            context = {
                'vendor': vendor,
                'doc_type': category
            }
            learned_folder = state["preference_repo"].get_preference(
                'vendor_folder',
                context,
                min_confidence=0.7
            )
        except Exception:
            pass  # å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
    
    if learned_folder:
        # Memory åå¥½å‘½ä¸­
        target_dir_rel = learned_folder
        state["routing_source"] = "memory"
        state["used_learned_preference"] = True
        print(f"   ğŸ§  è·¯ç”±: Memory åå¥½å‘½ä¸­ -> {target_dir_rel}")
    else:
        state["used_learned_preference"] = False
        
        # 2ï¸âƒ£ ä¼˜å…ˆçº§ 2: æ—¥æœŸåˆ†å±‚è§„åˆ™ (è´¢åŠ¡ç±»æ–‡æ¡£)
        if category in date_partition_types:
            target_dir_rel = build_date_partition_path(category, date_str)
            state["routing_source"] = "date_partition"
            print(f"   ğŸ“… è·¯ç”±: æ—¥æœŸåˆ†å±‚ ({category}) -> {target_dir_rel}")
        else:
            # 3ï¸âƒ£ ä¼˜å…ˆçº§ 3: è¯­ä¹‰åŒ–å½’æ¡£ (éè´¢åŠ¡ç±»æ–‡æ¡£)
            target_dir_rel = build_semantic_path(
                category=category,
                vendor=vendor,
                default_tpl=default_structure,
                fallback_tpl=fallback_structure
            )
            state["routing_source"] = "semantic"
            if is_valid_vendor(vendor):
                print(f"   ğŸ·ï¸  è·¯ç”±: è¯­ä¹‰åŒ– ({category}/{vendor}) -> {target_dir_rel}")
            else:
                print(f"   ğŸ“ è·¯ç”±: è¯­ä¹‰åŒ–å…œåº• ({category}/General) -> {target_dir_rel}")

    # æ‰©å±•åå¤„ç†
    ext = fp.suffix
    suggested = a["suggested_filename"]
    # å¦‚æœå»ºè®®åä¸åŒ…å«æ‰©å±•åï¼Œè¡¥ä¸Š
    if ext and (not suggested.lower().endswith(ext.lower())):
        new_name = f"{suggested}{ext.lower()}"
    else:
        new_name = suggested

    # åˆ›å»º Pydantic å¯¹è±¡
    plan = RenamePlan(
        category=category,
        new_name=new_name,
        dest_dir=target_dir_rel,
        confidence=a["confidence"],
        extracted={
            "date": date_str,
            "amount": a.get("extracted_amount"),
            "vendor_or_party": vendor,
            "title": a.get("title"),
        },
        rationale=a.get("rationale", ""),
    )
    state["plan"] = plan
    return state


def node_validate(state: JanitorState) -> JanitorState:
    """èŠ‚ç‚¹4: å®‰å…¨æ ¡éªŒ"""
    # å¤ç”¨ core.validator
    state["plan"] = validate_plan(state["plan"])
    return state


def node_human_review(state: JanitorState) -> JanitorState:
    """èŠ‚ç‚¹4.5: äººç±»åœ¨ç¯ç¡®è®¤ï¼ˆHITLï¼‰- Step 7 éé˜»å¡å¼å®¡æ‰¹ + OCR V2 è´¨é‡ç†”æ–­"""
    fp = state["file_path"]
    plan = state["plan"]

    # 1) ä¸åˆæ³•ï¼šè‡ªåŠ¨æ‹’ç»ï¼Œä¸è¯¢é—®
    if not plan.is_valid:
        state["approved"] = False
        state["decision"] = "auto_reject_invalid"
        return state

    # 2) åˆæ³•ï¼šæ‰“å°æ‘˜è¦
    print(f"\nğŸ§‘â€âš–ï¸  éœ€è¦ç¡®è®¤ï¼š{fp.name}")
    print(f"   â†’ æ–°åå­—ï¼š{plan.new_name}")
    print(f"   â†’ ç›®æ ‡ç›®å½•ï¼š{plan.dest_dir}")
    print(f"   â†’ ç±»åˆ«/ç½®ä¿¡åº¦ï¼š{plan.category} / {float(plan.confidence):.2f}")
    
    # ğŸ†• OCR V2: æ£€æŸ¥ OCR è´¨é‡ç†”æ–­
    extraction_metadata = state.get("extraction_metadata", {})
    ocr_needs_review = extraction_metadata.get("needs_review", False)
    ocr_quality_score = extraction_metadata.get("quality_score", 100)
    
    # ğŸ†• Step 7: éé˜»å¡å¼å®¡æ‰¹æœºåˆ¶
    auto_approve = state.get("auto_approve", False)
    
    # ğŸ†• OCR V2 è´¨é‡ç†”æ–­ï¼šå¦‚æœ OCR è´¨é‡ä½ï¼Œå¼ºåˆ¶è½¬ä¸ºäººå·¥å®¡æ‰¹
    if ocr_needs_review:
        print(f"   âš ï¸  OCR è´¨é‡ä½ ({ocr_quality_score}åˆ†)ï¼Œå¼ºåˆ¶è½¬ä¸ºäººå·¥å®¡æ‰¹")
        auto_approve = False  # å¼ºåˆ¶å…³é—­è‡ªåŠ¨æ‰¹å‡†
    
    if auto_approve:
        # è‡ªåŠ¨æ‰¹å‡†æ¨¡å¼ï¼šç«‹å³æ‰¹å‡†
        print("   ğŸ¤– è‡ªåŠ¨æ‰¹å‡†æ¨¡å¼ï¼šå·²æ‰¹å‡†")
        state["approved"] = True
        state["decision"] = "approved"
    else:
        # éè‡ªåŠ¨æ‰¹å‡†æ¨¡å¼ï¼šä¿å­˜ä¸ºå¾…å®¡æ‰¹ JSON
        import json
        from datetime import datetime
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:20]
        safe_filename = fp.stem.replace(" ", "_")[:30]  # é™åˆ¶é•¿åº¦ï¼Œé¿å…è·¯å¾„è¿‡é•¿
        pending_filename = f"plan_{timestamp}_{safe_filename}.json"
        pending_path = Path("pending") / pending_filename
        
        # æ„å»ºå¾…å®¡æ‰¹æ•°æ®
        pending_data = {
            "original_file": str(fp),
            "original_name": fp.name,
            "new_name": plan.new_name,
            "dest_dir": plan.dest_dir,
            "category": plan.category,
            "confidence": float(plan.confidence),
            "extracted": plan.extracted,
            "rationale": plan.rationale,
            "preview": state.get("preview", "")[:500],  # ä¿å­˜å‰500å­—ç¬¦é¢„è§ˆ
            "created_at": datetime.now().isoformat(),
            "status": "pending",
            # ğŸ†• OCR V2: è®°å½•è´¨é‡é—®é¢˜æ ‡è®°
            "ocr_quality_issue": ocr_needs_review,
            "ocr_quality_score": ocr_quality_score,
            "extraction_method": extraction_metadata.get("method", "unknown"),
        }
        
        # ä¿å­˜åˆ° JSON æ–‡ä»¶
        with pending_path.open("w", encoding="utf-8") as f:
            json.dump(pending_data, f, ensure_ascii=False, indent=2)
        
        print(f"   â³ è®¡åˆ’å·²ç”Ÿæˆï¼Œç­‰å¾… UI å®¡æ‰¹")
        print(f"      æ–‡ä»¶ï¼š{pending_filename}")
        
        # ğŸ†• OCR V2: å¦‚æœæ˜¯å› ä¸º OCR è´¨é‡é—®é¢˜å¯¼è‡´çš„äººå·¥å®¡æ‰¹ï¼Œé¢å¤–æç¤º
        if ocr_needs_review:
            print(f"      âš ï¸  åŸå› ï¼šOCR è´¨é‡ä½ ({ocr_quality_score}åˆ†)")
        
        # è®¾ç½®ä¸º pending çŠ¶æ€ï¼ˆä¸æ‰§è¡Œï¼Œä¹Ÿä¸æ‹’ç»ï¼‰
        state["approved"] = False
        state["decision"] = "pending"
        state["pending_file"] = str(pending_path)
    
    return state


def node_apply(state: JanitorState) -> JanitorState:
    """èŠ‚ç‚¹5a: æ‰§è¡Œï¼ˆæ‰¹å‡†åï¼‰"""
    fp = state["file_path"]
    plan = state["plan"]
    archive_root = state["archive_root"]

    # å†ä¿é™©ï¼šå¦‚æœæ²¡æ‰¹å‡†ï¼Œç›´æ¥è¿”å›
    if not state.get("approved", False):
        return state

    # ğŸ†• Step 5: åŒºåˆ† Dry-run å’ŒçœŸå®æ‰§è¡Œ
    if state.get("dry_run", True):
        # Dry-run æ¨¡å¼ï¼šåªæ‰“å°ï¼Œä¸åŠ¨æ–‡ä»¶
        print(f"   ğŸ§ª Dry-run æ‰¹å‡†ï¼šå°†æŠŠ {fp.name} â†’ {plan.new_name}")
        print(f"      ç§»åŠ¨åˆ° {plan.dest_dir}\n")
        
        execution_status = "dry_run"
        moved_to = None
        
    else:
        # ğŸ†• çœŸå®æ‰§è¡Œæ¨¡å¼ï¼šå®é™…ç§»åŠ¨æ–‡ä»¶
        # æ„å»ºå®Œæ•´çš„ç›®æ ‡è·¯å¾„
        dst = archive_root / plan.dest_dir / plan.new_name
        
        # è°ƒç”¨ safe_move_file æ‰§è¡Œç§»åŠ¨
        move_result = safe_move_file(fp, dst)
        state["move_result"] = move_result
        
        if move_result["status"] == "success":
            # ç§»åŠ¨æˆåŠŸ
            execution_status = "success"
            moved_to = move_result["dst"]
            
            print(f"   ğŸ“¦ å·²ç§»åŠ¨ï¼š{fp.name}")
            print(f"      â†’ {moved_to}")
            
            # å¦‚æœå‘ç”Ÿäº†å†²çªè§£å†³
            if move_result.get("conflict_resolved", False):
                final_name = Path(moved_to).name
                print(f"      âš ï¸  æ£€æµ‹åˆ°å†²çªï¼Œå·²é‡å‘½åä¸º {final_name}")
            
            print()
        else:
            # ç§»åŠ¨å¤±è´¥
            execution_status = "failed"
            moved_to = None
            
            print(f"   âŒ ç§»åŠ¨å¤±è´¥ï¼š{fp.name}")
            print(f"      åŸå› : {move_result.get('error', 'æœªçŸ¥é”™è¯¯')}\n")

    # ğŸ†• OCR V2: è·å–æå–å…ƒæ•°æ®
    extraction_metadata = state.get("extraction_metadata", {})
    
    # æ„å»ºæ—¥å¿—è®°å½•
    state["record"] = {
        "timestamp": datetime.now().isoformat(),
        "original_file": str(fp),
        "file_size_mb": get_file_size_mb(fp),
        "preview": (state.get("preview", "")[:200]),
        "plan": plan.model_dump(),
        "dry_run": state.get("dry_run", True),
        "approved": state.get("approved", False),
        "decision": state.get("decision", "no_decision"),
        # ğŸ†• Step 5: è®°å½•æ‰§è¡Œç»“æœ
        "execution_status": execution_status,
        "moved_to": moved_to,
        "move_result": state.get("move_result"),
        # ğŸ†• OCR V2: è®°å½•æ–‡æœ¬æå–å…ƒæ•°æ®
        "extraction_metadata": extraction_metadata,
        # ğŸ†• PR#2: è®°å½•è·¯ç”±æ¥æº
        "routing_source": state.get("routing_source", "unknown"),
        "used_learned_preference": state.get("used_learned_preference", False),
    }
    return state


def node_skip(state: JanitorState) -> JanitorState:
    """èŠ‚ç‚¹5b: è·³è¿‡ï¼ˆæ‹’ç»/ä¸åˆæ³•/å¾…å®¡æ‰¹ï¼‰"""
    fp = state["file_path"]
    plan = state["plan"]
    decision = state.get("decision", "skipped")

    # ğŸ†• OCR V2: è·å–æå–å…ƒæ•°æ®
    extraction_metadata = state.get("extraction_metadata", {})
    ocr_needs_review = extraction_metadata.get("needs_review", False)
    ocr_quality_score = extraction_metadata.get("quality_score", 100)

    # æ ¹æ®å†³ç­–ç±»å‹ç»™å‡ºä¸åŒçš„æç¤º
    if decision == "auto_reject_invalid":
        emoji = "âŒ"
        reason = f"ä¸åˆæ³•: {plan.validation_msg}"
    elif decision == "rejected":
        emoji = "â­ï¸"
        reason = "ç”¨æˆ·æ‹’ç»"
    elif decision == "pending":
        emoji = "â³"
        reason = "ç­‰å¾…å®¡æ‰¹"
        # ğŸ†• OCR V2: å¦‚æœå›  OCR è´¨é‡é—®é¢˜å¯¼è‡´çš„ pendingï¼Œåœ¨åŸå› ä¸­æ³¨æ˜
        if ocr_needs_review:
            reason = f"ç­‰å¾…å®¡æ‰¹ (OCRè´¨é‡ä½: {ocr_quality_score}åˆ†)"
    else:
        emoji = "â­ï¸"
        reason = "è·³è¿‡"
    
    print(f"   {emoji} {reason}ï¼š{fp.name}\n")

    # æ„å»ºæ—¥å¿—è®°å½•ï¼ˆå³ä½¿è·³è¿‡ä¹Ÿè¦è®°å½•ï¼‰
    state["record"] = {
        "timestamp": datetime.now().isoformat(),
        "original_file": str(fp),
        "file_size_mb": get_file_size_mb(fp),
        "preview": (state.get("preview", "")[:200]),
        "plan": plan.model_dump(),
        "dry_run": state.get("dry_run", True),
        "approved": False,
        "decision": decision,
        "pending_file": state.get("pending_file"),  # ğŸ†• Step 7: è®°å½•å¾…å®¡æ‰¹æ–‡ä»¶è·¯å¾„
        # ğŸ†• OCR V2: è®°å½•æ–‡æœ¬æå–å…ƒæ•°æ®
        "extraction_metadata": extraction_metadata,
        # ğŸ†• PR#2: è®°å½•è·¯ç”±æ¥æº
        "routing_source": state.get("routing_source", "unknown"),
        "used_learned_preference": state.get("used_learned_preference", False),
    }
    return state


# --- è·¯ç”±å‡½æ•° ---
def route_after_review(state: JanitorState) -> str:
    """æ ¹æ®äººç±»å†³ç­–è·¯ç”±åˆ°ä¸åŒèŠ‚ç‚¹"""
    return "apply" if state.get("approved") else "skip"


# --- 3. æ„å»ºå›¾ (Graph) ---
def build_graph():
    """æ„å»º LangGraph çŠ¶æ€å›¾"""

    # æ„å»ºçŠ¶æ€å›¾
    g = StateGraph(JanitorState)
    
    # æ·»åŠ èŠ‚ç‚¹
    g.add_node("extract_preview", node_extract_preview)
    g.add_node("llm_analyze", node_llm_analyze)
    g.add_node("build_plan", node_build_plan)
    g.add_node("validate", node_validate)
    g.add_node("human_review", node_human_review)  # ğŸ†• æ–°å¢
    g.add_node("apply", node_apply)                # ğŸ†• æ‹†åˆ†åçš„èŠ‚ç‚¹
    g.add_node("skip", node_skip)                  # ğŸ†• æ‹†åˆ†åçš„èŠ‚ç‚¹

    # å®šä¹‰è¾¹ (Edge)
    g.set_entry_point("extract_preview")
    g.add_edge("extract_preview", "llm_analyze")
    g.add_edge("llm_analyze", "build_plan")
    g.add_edge("build_plan", "validate")
    g.add_edge("validate", "human_review")         # ğŸ†• validate åè¿›å…¥äººç±»ç¡®è®¤

    # ğŸ†• æ¡ä»¶åˆ†æ”¯ï¼šæ ¹æ®äººç±»å†³ç­–è·¯ç”±
    g.add_conditional_edges(
        "human_review",
        route_after_review,
        {"apply": "apply", "skip": "skip"}
    )

    g.add_edge("apply", END)
    g.add_edge("skip", END)

    return g.compile()


# --- è¾…åŠ©å‡½æ•° ---
def load_config(path: Path) -> dict:
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)


# ==================== Step 6 Phase 1: å¯å¤ç”¨çš„å·¥ä½œæµç±» ====================

class JanitorWorkflow:
    """
    Digital Janitor å·¥ä½œæµå°è£…ç±»
    
    å°†æ ¸å¿ƒé€»è¾‘å°è£…ï¼Œä½¿å…¶æ—¢èƒ½è¢«å‘½ä»¤è¡Œè°ƒç”¨ï¼Œä¹Ÿèƒ½è¢«å…¶ä»–è„šæœ¬ï¼ˆå¦‚ watcherï¼‰å¯¼å…¥ä½¿ç”¨ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        # åˆå§‹åŒ–å·¥ä½œæµ
        workflow = JanitorWorkflow(config_path="config.yaml", env_path=".env")
        
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        result = workflow.process_file(
            file_path=Path("inbox/document.pdf"),
            dry_run=False,
            auto_approve=False,
            max_preview=1000
        )
    """
    
    def __init__(self, config_path: str = "config.yaml", env_path: str = ".env"):
        """
        åˆå§‹åŒ–å·¥ä½œæµ
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            env_path: ç¯å¢ƒå˜é‡æ–‡ä»¶è·¯å¾„
        """
        # 1. åŠ è½½ç¯å¢ƒå˜é‡
        env_file = Path(env_path)
        if env_file.exists():
            load_dotenv(env_file)
        
        # 2. åŠ è½½é…ç½®
        self.config_path = Path(config_path)
        self.cfg = load_config(self.config_path)
        
        # 3. è§£æè·¯å¾„é…ç½®
        self.inbox = Path(self.cfg["paths"]["inbox"])
        self.archive = Path(self.cfg["paths"]["archive"])
        self.logs = Path(self.cfg["paths"]["logs"])
        self.pending = Path("pending")  # ğŸ†• Step 7: å¾…å®¡æ‰¹ç›®å½•
        
        # 4. ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
        self.pending.mkdir(parents=True, exist_ok=True)
        
        # 5. ç¼–è¯‘ LangGraph å›¾ï¼ˆåªç¼–è¯‘ä¸€æ¬¡ï¼Œé‡å¤ä½¿ç”¨ï¼‰
        self.app = build_graph()
        
        # 6. ğŸ†• åˆå§‹åŒ– Memory ç³»ç»Ÿ
        self.memory_db = MemoryDatabase()
        self.approval_repo = ApprovalRepository(self.memory_db)
        self.preference_repo = PreferenceRepository(self.memory_db)
        self.session_id = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    def process_file(
        self, 
        file_path: Path, 
        dry_run: bool = True,
        auto_approve: bool = False,
        max_preview: int = 1000
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„
            dry_run: æ˜¯å¦ä¸º dry-run æ¨¡å¼ï¼ˆåªé¢„è§ˆï¼Œä¸å®é™…ç§»åŠ¨ï¼‰
            auto_approve: æ˜¯å¦è‡ªåŠ¨æ‰¹å‡†ï¼ˆè·³è¿‡äººå·¥ç¡®è®¤ï¼‰
            max_preview: LLM åˆ†æçš„æœ€å¤§æ–‡æœ¬é•¿åº¦
        
        Returns:
            åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸ï¼ˆrecordï¼‰
            
        Example:
            >>> workflow = JanitorWorkflow()
            >>> result = workflow.process_file(
            ...     Path("inbox/contract.pdf"),
            ...     dry_run=False,
            ...     auto_approve=True
            ... )
            >>> print(result["execution_status"])
            success
        """
        # æ„é€ åˆå§‹çŠ¶æ€
        initial_state: JanitorState = {
            "file_path": file_path,
            "cfg": self.cfg,
            "archive_root": self.archive,
            "dry_run": dry_run,
            "max_preview": max_preview,
            "auto_approve": auto_approve,
            "preference_repo": self.preference_repo,  # ğŸ†• ä¼ å…¥åå¥½ä»“åº“
        }
        
        # è°ƒç”¨ç¼–è¯‘å¥½çš„å›¾æ‰§è¡Œå·¥ä½œæµ
        final_state = self.app.invoke(initial_state)
        
        # è¿”å›å¤„ç†è®°å½•
        return final_state.get("record", {})
    
    def _compute_file_hash(self, file_path: Path) -> str:
        """
        è®¡ç®—æ–‡ä»¶ hashï¼ˆå¿«é€Ÿç®—æ³•ï¼šæ–‡ä»¶å¤§å° + å¤´éƒ¨ 8KBï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            SHA256 å“ˆå¸Œå­—ç¬¦ä¸²
        """
        hasher = hashlib.sha256()
        file_size = file_path.stat().st_size
        
        # 1. å†™å…¥æ–‡ä»¶å¤§å°
        hasher.update(str(file_size).encode('utf-8'))
        
        # 2. è¯»å–å¤´éƒ¨ 8KB
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(8192)
                hasher.update(chunk)
        except Exception:
            pass
        
        return hasher.hexdigest()
    
    def get_learned_folder(self, vendor: str, doc_type: str) -> Optional[str]:
        """
        è·å–å­¦ä¹ åˆ°çš„æ–‡ä»¶å¤¹åå¥½
        
        Args:
            vendor: ä¾›åº”å•†åç§°
            doc_type: æ–‡æ¡£ç±»å‹
        
        Returns:
            å­¦ä¹ åˆ°çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        context = {
            'vendor': vendor,
            'doc_type': doc_type
        }
        return self.preference_repo.get_preference(
            'vendor_folder',
            context,
            min_confidence=0.7
        )
    
    def save_approval_decision(
        self,
        file_path: Path,
        analysis: Dict[str, Any],
        plan: Dict[str, Any],
        final_filename: str,
        final_folder: str,
        action: str,
        processing_time_ms: int = 0,
        extraction_method: str = "unknown"
    ):
        """
        ä¿å­˜ç”¨æˆ·çš„å®¡æ‰¹å†³ç­–åˆ° Memory ç³»ç»Ÿ
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            analysis: LLM åˆ†æç»“æœ
            plan: é‡å‘½åè®¡åˆ’
            final_filename: æœ€ç»ˆæ–‡ä»¶å
            final_folder: æœ€ç»ˆæ–‡ä»¶å¤¹
            action: æ“ä½œç±»å‹ (approved/modified/rejected/skipped)
            processing_time_ms: å¤„ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
            extraction_method: æ–‡æœ¬æå–æ–¹æ³•
        """
        try:
            # è®¡ç®—æ–‡ä»¶ hash
            file_hash = self._compute_file_hash(file_path)
            
            # å‡†å¤‡æ—¥å¿—æ•°æ®
            log_data = {
                'session_id': self.session_id,
                'file_hash': file_hash,
                'original_filename': file_path.name,
                'original_path': str(file_path),
                'file_size_bytes': file_path.stat().st_size,
                
                # AI åˆ†æ
                'doc_type': analysis.get('category'),
                'vendor': analysis.get('vendor_or_party'),
                'extracted_date': analysis.get('extracted_date'),
                'confidence_score': analysis.get('confidence', 0.0),
                
                # å»ºè®® vs å®é™…
                'suggested_filename': plan.get('new_name', ''),
                'suggested_folder': plan.get('dest_dir', ''),
                'final_filename': final_filename,
                'final_folder': final_folder,
                
                # å†³ç­–
                'action': action,
                'user_modified_filename': final_filename != plan.get('new_name'),
                'user_modified_folder': final_folder != plan.get('dest_dir'),
                
                # å¤„ç†ä¿¡æ¯
                'processing_time_ms': processing_time_ms,
                'extraction_method': extraction_method
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self.approval_repo.save_approval(log_data)
            
        except Exception as e:
            # è®°å½•å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            print(f"âš ï¸  Failed to save approval decision: {e}")


# --- ä¸»å‡½æ•° ---

def main():
    """
    å‘½ä»¤è¡Œå…¥å£å‡½æ•°ï¼ˆé‡æ„åï¼‰
    ä½¿ç”¨ JanitorWorkflow ç±»æ¥å¤„ç†æ–‡ä»¶
    """
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    ap = argparse.ArgumentParser(description="Digital Janitor - LangGraph Workflow")
    ap.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    ap.add_argument("--env", type=str, default=".env", help="ç¯å¢ƒå˜é‡æ–‡ä»¶è·¯å¾„")
    ap.add_argument("--preview", type=int, default=1000, help="LLM åˆ†æçš„æœ€å¤§æ–‡æœ¬é•¿åº¦")
    ap.add_argument("--limit", type=int, default=10, help="å¤„ç†æ–‡ä»¶æ•°é‡é™åˆ¶")
    ap.add_argument("--auto-approve", action="store_true", help="è‡ªåŠ¨æ‰¹å‡†æ‰€æœ‰åˆæ³•è®¡åˆ’ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    ap.add_argument("--execute", action="store_true", help="ğŸ†• çœŸå®æ‰§è¡Œæ¨¡å¼ï¼ˆç§»åŠ¨æ–‡ä»¶ï¼‰ï¼Œé»˜è®¤ä¸º dry-run")
    args = ap.parse_args()

    # 2. åˆå§‹åŒ–å·¥ä½œæµï¼ˆåŠ è½½é…ç½®ã€ç¼–è¯‘å›¾ï¼‰
    try:
        workflow = JanitorWorkflow(
            config_path=args.config,
            env_path=args.env
        )
    except Exception as e:
        print(f"[ERROR] Failed to initialize workflow: {e}")
        return 1

    # 3. å‡†å¤‡ç›®å½•å’Œæ‰«ææ–‡ä»¶
    workflow.inbox.mkdir(parents=True, exist_ok=True)
    workflow.logs.mkdir(parents=True, exist_ok=True)
    
    files = discover_files(workflow.inbox)[: args.limit]
    if not files:
        print(f"[WARN] Inbox is empty: {workflow.inbox}")
        return 0

    # 4. æ‰“å°å¯åŠ¨ä¿¡æ¯
    dry_run = not args.execute
    mode_str = "ğŸ§ª DRY-RUN æ¨¡å¼ï¼ˆä¸ä¼šç§»åŠ¨æ–‡ä»¶ï¼‰" if dry_run else "âš¡ EXECUTE æ¨¡å¼ï¼ˆçœŸå®ç§»åŠ¨æ–‡ä»¶ï¼‰"
    print(f"ğŸš€ Starting LangGraph Workflow with HITL")
    print(f"ğŸ“‚ Processing {len(files)} files...")
    print(f"ğŸ”§ Mode: {mode_str}\n")
    print("=" * 80)

    # 5. é€ä¸ªå¤„ç†æ–‡ä»¶
    records = []
    for i, fp in enumerate(files, 1):
        print(f"\n[{i}/{len(files)}] ğŸ” Processing: {fp.name}")
        print("-" * 80)
        
        # ğŸ†• Step 6: ä½¿ç”¨ workflow.process_file() æ–¹æ³•
        record = workflow.process_file(
            file_path=fp,
            dry_run=dry_run,
            auto_approve=args.auto_approve,
            max_preview=args.preview
        )
        records.append(record)

    # 6. ä¿å­˜æ—¥å¿—
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    mode_suffix = "dryrun" if dry_run else "execute"
    log_path = workflow.logs / f"graph_plan_{mode_suffix}_{ts}.jsonl"
    with log_path.open("w", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
            
    print("-" * 80)
    print(f"âœ… Completed. Log saved to: {log_path.name}")
    if not dry_run:
        print(f"ğŸ“¦ Files have been moved to: {workflow.archive.absolute()}")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())